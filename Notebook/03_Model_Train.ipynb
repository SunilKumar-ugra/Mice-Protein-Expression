{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "# to visualise al the columns in the dataframe\n",
    "pd.pandas.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MouseID</th>\n",
       "      <th>cls</th>\n",
       "      <th>DYRK1A_N</th>\n",
       "      <th>BDNF_N</th>\n",
       "      <th>NR1_N</th>\n",
       "      <th>NR2A_N</th>\n",
       "      <th>pAKT_N</th>\n",
       "      <th>pBRAF_N</th>\n",
       "      <th>pCAMKII_N</th>\n",
       "      <th>pCREB_N</th>\n",
       "      <th>pELK_N</th>\n",
       "      <th>pJNK_N</th>\n",
       "      <th>PKCA_N</th>\n",
       "      <th>pMEK_N</th>\n",
       "      <th>pNR2A_N</th>\n",
       "      <th>pPKCAB_N</th>\n",
       "      <th>pRSK_N</th>\n",
       "      <th>AKT_N</th>\n",
       "      <th>CAMKII_N</th>\n",
       "      <th>CREB_N</th>\n",
       "      <th>ELK_N</th>\n",
       "      <th>ERK_N</th>\n",
       "      <th>GSK3B_N</th>\n",
       "      <th>JNK_N</th>\n",
       "      <th>MEK_N</th>\n",
       "      <th>TRKA_N</th>\n",
       "      <th>RSK_N</th>\n",
       "      <th>APP_N</th>\n",
       "      <th>SOD1_N</th>\n",
       "      <th>MTOR_N</th>\n",
       "      <th>P38_N</th>\n",
       "      <th>pMTOR_N</th>\n",
       "      <th>DSCR1_N</th>\n",
       "      <th>AMPKA_N</th>\n",
       "      <th>NR2B_N</th>\n",
       "      <th>pNUMB_N</th>\n",
       "      <th>RAPTOR_N</th>\n",
       "      <th>TIAM1_N</th>\n",
       "      <th>pP70S6_N</th>\n",
       "      <th>NUMB_N</th>\n",
       "      <th>P70S6_N</th>\n",
       "      <th>pGSK3B_N</th>\n",
       "      <th>pPKCG_N</th>\n",
       "      <th>CDK5_N</th>\n",
       "      <th>S6_N</th>\n",
       "      <th>ADARB1_N</th>\n",
       "      <th>AcetylH3K9_N</th>\n",
       "      <th>RRP1_N</th>\n",
       "      <th>BAX_N</th>\n",
       "      <th>ARC_N</th>\n",
       "      <th>ERBB4_N</th>\n",
       "      <th>nNOS_N</th>\n",
       "      <th>Tau_N</th>\n",
       "      <th>GFAP_N</th>\n",
       "      <th>GluR3_N</th>\n",
       "      <th>GluR4_N</th>\n",
       "      <th>IL1B_N</th>\n",
       "      <th>P3525_N</th>\n",
       "      <th>pCASP9_N</th>\n",
       "      <th>PSD95_N</th>\n",
       "      <th>SNCA_N</th>\n",
       "      <th>Ubiquitin_N</th>\n",
       "      <th>pGSK3B_Tyr216_N</th>\n",
       "      <th>SHH_N</th>\n",
       "      <th>BAD_N</th>\n",
       "      <th>BCL2_N</th>\n",
       "      <th>pCFOS_N</th>\n",
       "      <th>SYP_N</th>\n",
       "      <th>H3AcK18_N</th>\n",
       "      <th>EGR1_N</th>\n",
       "      <th>H3MeK4_N</th>\n",
       "      <th>CaNA_N</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>309_1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.151122</td>\n",
       "      <td>0.824638</td>\n",
       "      <td>0.612119</td>\n",
       "      <td>0.630482</td>\n",
       "      <td>0.327006</td>\n",
       "      <td>0.448666</td>\n",
       "      <td>0.168257</td>\n",
       "      <td>0.617322</td>\n",
       "      <td>0.232553</td>\n",
       "      <td>0.576167</td>\n",
       "      <td>0.747688</td>\n",
       "      <td>0.598504</td>\n",
       "      <td>0.286697</td>\n",
       "      <td>0.698164</td>\n",
       "      <td>0.622784</td>\n",
       "      <td>0.711198</td>\n",
       "      <td>0.419649</td>\n",
       "      <td>0.317149</td>\n",
       "      <td>0.593715</td>\n",
       "      <td>0.627907</td>\n",
       "      <td>0.596269</td>\n",
       "      <td>0.640167</td>\n",
       "      <td>0.643070</td>\n",
       "      <td>0.766146</td>\n",
       "      <td>0.295596</td>\n",
       "      <td>0.549817</td>\n",
       "      <td>0.092035</td>\n",
       "      <td>0.541196</td>\n",
       "      <td>0.152338</td>\n",
       "      <td>0.687269</td>\n",
       "      <td>0.553922</td>\n",
       "      <td>0.467278</td>\n",
       "      <td>0.509996</td>\n",
       "      <td>0.469461</td>\n",
       "      <td>0.436170</td>\n",
       "      <td>0.506018</td>\n",
       "      <td>0.163368</td>\n",
       "      <td>0.323059</td>\n",
       "      <td>0.373254</td>\n",
       "      <td>0.604516</td>\n",
       "      <td>0.303363</td>\n",
       "      <td>0.178458</td>\n",
       "      <td>0.324085</td>\n",
       "      <td>0.402810</td>\n",
       "      <td>0.083584</td>\n",
       "      <td>0.327870</td>\n",
       "      <td>0.690257</td>\n",
       "      <td>0.426816</td>\n",
       "      <td>0.412721</td>\n",
       "      <td>0.477834</td>\n",
       "      <td>0.057168</td>\n",
       "      <td>0.228825</td>\n",
       "      <td>0.531163</td>\n",
       "      <td>0.151103</td>\n",
       "      <td>0.242608</td>\n",
       "      <td>0.169702</td>\n",
       "      <td>0.432843</td>\n",
       "      <td>0.483783</td>\n",
       "      <td>0.044770</td>\n",
       "      <td>0.256699</td>\n",
       "      <td>0.405228</td>\n",
       "      <td>0.162941</td>\n",
       "      <td>0.177312</td>\n",
       "      <td>0.269902</td>\n",
       "      <td>0.133930</td>\n",
       "      <td>0.336299</td>\n",
       "      <td>0.087715</td>\n",
       "      <td>0.102890</td>\n",
       "      <td>0.084580</td>\n",
       "      <td>0.705738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>309_2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.155750</td>\n",
       "      <td>0.776455</td>\n",
       "      <td>0.601070</td>\n",
       "      <td>0.585247</td>\n",
       "      <td>0.311887</td>\n",
       "      <td>0.429899</td>\n",
       "      <td>0.154925</td>\n",
       "      <td>0.590173</td>\n",
       "      <td>0.205362</td>\n",
       "      <td>0.559556</td>\n",
       "      <td>0.688545</td>\n",
       "      <td>0.559598</td>\n",
       "      <td>0.270688</td>\n",
       "      <td>0.591606</td>\n",
       "      <td>0.629311</td>\n",
       "      <td>0.689095</td>\n",
       "      <td>0.383669</td>\n",
       "      <td>0.291583</td>\n",
       "      <td>0.548032</td>\n",
       "      <td>0.578736</td>\n",
       "      <td>0.584234</td>\n",
       "      <td>0.614355</td>\n",
       "      <td>0.586180</td>\n",
       "      <td>0.724593</td>\n",
       "      <td>0.251840</td>\n",
       "      <td>0.491969</td>\n",
       "      <td>0.075589</td>\n",
       "      <td>0.467650</td>\n",
       "      <td>0.137451</td>\n",
       "      <td>0.621033</td>\n",
       "      <td>0.512117</td>\n",
       "      <td>0.409897</td>\n",
       "      <td>0.457694</td>\n",
       "      <td>0.410046</td>\n",
       "      <td>0.383101</td>\n",
       "      <td>0.447496</td>\n",
       "      <td>0.145594</td>\n",
       "      <td>0.322736</td>\n",
       "      <td>0.376914</td>\n",
       "      <td>0.618918</td>\n",
       "      <td>0.302059</td>\n",
       "      <td>0.177452</td>\n",
       "      <td>0.324004</td>\n",
       "      <td>0.386524</td>\n",
       "      <td>0.084514</td>\n",
       "      <td>0.326426</td>\n",
       "      <td>0.664894</td>\n",
       "      <td>0.429952</td>\n",
       "      <td>0.463251</td>\n",
       "      <td>0.488028</td>\n",
       "      <td>0.075103</td>\n",
       "      <td>0.251911</td>\n",
       "      <td>0.576828</td>\n",
       "      <td>0.149555</td>\n",
       "      <td>0.285860</td>\n",
       "      <td>0.212421</td>\n",
       "      <td>0.472327</td>\n",
       "      <td>0.477640</td>\n",
       "      <td>0.054452</td>\n",
       "      <td>0.226088</td>\n",
       "      <td>0.433471</td>\n",
       "      <td>0.220010</td>\n",
       "      <td>0.146494</td>\n",
       "      <td>0.269902</td>\n",
       "      <td>0.110434</td>\n",
       "      <td>0.365208</td>\n",
       "      <td>0.080692</td>\n",
       "      <td>0.115874</td>\n",
       "      <td>0.093977</td>\n",
       "      <td>0.749771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>309_3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.153459</td>\n",
       "      <td>0.793572</td>\n",
       "      <td>0.558911</td>\n",
       "      <td>0.575910</td>\n",
       "      <td>0.306369</td>\n",
       "      <td>0.441381</td>\n",
       "      <td>0.153485</td>\n",
       "      <td>0.607102</td>\n",
       "      <td>0.199194</td>\n",
       "      <td>0.541938</td>\n",
       "      <td>0.670905</td>\n",
       "      <td>0.560573</td>\n",
       "      <td>0.283848</td>\n",
       "      <td>0.581558</td>\n",
       "      <td>0.669753</td>\n",
       "      <td>0.670907</td>\n",
       "      <td>0.415579</td>\n",
       "      <td>0.292676</td>\n",
       "      <td>0.549983</td>\n",
       "      <td>0.599925</td>\n",
       "      <td>0.580790</td>\n",
       "      <td>0.625756</td>\n",
       "      <td>0.613502</td>\n",
       "      <td>0.730384</td>\n",
       "      <td>0.270560</td>\n",
       "      <td>0.472444</td>\n",
       "      <td>0.076445</td>\n",
       "      <td>0.470688</td>\n",
       "      <td>0.137475</td>\n",
       "      <td>0.616142</td>\n",
       "      <td>0.510175</td>\n",
       "      <td>0.375653</td>\n",
       "      <td>0.467725</td>\n",
       "      <td>0.400225</td>\n",
       "      <td>0.356363</td>\n",
       "      <td>0.432379</td>\n",
       "      <td>0.125773</td>\n",
       "      <td>0.334325</td>\n",
       "      <td>0.383316</td>\n",
       "      <td>0.657565</td>\n",
       "      <td>0.332565</td>\n",
       "      <td>0.189744</td>\n",
       "      <td>0.369553</td>\n",
       "      <td>0.373234</td>\n",
       "      <td>0.094486</td>\n",
       "      <td>0.312439</td>\n",
       "      <td>0.700212</td>\n",
       "      <td>0.448652</td>\n",
       "      <td>0.415863</td>\n",
       "      <td>0.475009</td>\n",
       "      <td>0.071718</td>\n",
       "      <td>0.248192</td>\n",
       "      <td>0.607535</td>\n",
       "      <td>0.150434</td>\n",
       "      <td>0.373879</td>\n",
       "      <td>0.202733</td>\n",
       "      <td>0.467603</td>\n",
       "      <td>0.484953</td>\n",
       "      <td>0.044526</td>\n",
       "      <td>0.214719</td>\n",
       "      <td>0.429387</td>\n",
       "      <td>0.186816</td>\n",
       "      <td>0.155920</td>\n",
       "      <td>0.269902</td>\n",
       "      <td>0.121560</td>\n",
       "      <td>0.353621</td>\n",
       "      <td>0.080465</td>\n",
       "      <td>0.109050</td>\n",
       "      <td>0.082162</td>\n",
       "      <td>0.868229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>309_4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.125169</td>\n",
       "      <td>0.637326</td>\n",
       "      <td>0.468152</td>\n",
       "      <td>0.480646</td>\n",
       "      <td>0.335530</td>\n",
       "      <td>0.444307</td>\n",
       "      <td>0.132074</td>\n",
       "      <td>0.486945</td>\n",
       "      <td>0.205135</td>\n",
       "      <td>0.554293</td>\n",
       "      <td>0.657048</td>\n",
       "      <td>0.640641</td>\n",
       "      <td>0.211238</td>\n",
       "      <td>0.627585</td>\n",
       "      <td>0.687775</td>\n",
       "      <td>0.593408</td>\n",
       "      <td>0.401866</td>\n",
       "      <td>0.319599</td>\n",
       "      <td>0.342080</td>\n",
       "      <td>0.452058</td>\n",
       "      <td>0.545716</td>\n",
       "      <td>0.625528</td>\n",
       "      <td>0.492220</td>\n",
       "      <td>0.667283</td>\n",
       "      <td>0.277208</td>\n",
       "      <td>0.440780</td>\n",
       "      <td>0.076936</td>\n",
       "      <td>0.479533</td>\n",
       "      <td>0.144945</td>\n",
       "      <td>0.605654</td>\n",
       "      <td>0.514305</td>\n",
       "      <td>0.338199</td>\n",
       "      <td>0.461189</td>\n",
       "      <td>0.406715</td>\n",
       "      <td>0.402786</td>\n",
       "      <td>0.422989</td>\n",
       "      <td>0.267936</td>\n",
       "      <td>0.220414</td>\n",
       "      <td>0.311501</td>\n",
       "      <td>0.549444</td>\n",
       "      <td>0.364189</td>\n",
       "      <td>0.181101</td>\n",
       "      <td>0.231762</td>\n",
       "      <td>0.333033</td>\n",
       "      <td>0.076248</td>\n",
       "      <td>0.338264</td>\n",
       "      <td>0.669358</td>\n",
       "      <td>0.392700</td>\n",
       "      <td>0.372772</td>\n",
       "      <td>0.397939</td>\n",
       "      <td>0.053257</td>\n",
       "      <td>0.245676</td>\n",
       "      <td>0.562594</td>\n",
       "      <td>0.156083</td>\n",
       "      <td>0.242672</td>\n",
       "      <td>0.184790</td>\n",
       "      <td>0.364359</td>\n",
       "      <td>0.449304</td>\n",
       "      <td>0.119259</td>\n",
       "      <td>0.208943</td>\n",
       "      <td>0.407971</td>\n",
       "      <td>0.179047</td>\n",
       "      <td>0.229602</td>\n",
       "      <td>0.269902</td>\n",
       "      <td>0.151031</td>\n",
       "      <td>0.265619</td>\n",
       "      <td>0.126763</td>\n",
       "      <td>0.164241</td>\n",
       "      <td>0.144543</td>\n",
       "      <td>0.721879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>309_5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.122146</td>\n",
       "      <td>0.637787</td>\n",
       "      <td>0.426467</td>\n",
       "      <td>0.441977</td>\n",
       "      <td>0.314976</td>\n",
       "      <td>0.433100</td>\n",
       "      <td>0.129086</td>\n",
       "      <td>0.410194</td>\n",
       "      <td>0.189152</td>\n",
       "      <td>0.532161</td>\n",
       "      <td>0.608970</td>\n",
       "      <td>0.551236</td>\n",
       "      <td>0.200364</td>\n",
       "      <td>0.579839</td>\n",
       "      <td>0.698127</td>\n",
       "      <td>0.557701</td>\n",
       "      <td>0.381348</td>\n",
       "      <td>0.297196</td>\n",
       "      <td>0.358746</td>\n",
       "      <td>0.433909</td>\n",
       "      <td>0.519977</td>\n",
       "      <td>0.599623</td>\n",
       "      <td>0.471524</td>\n",
       "      <td>0.627685</td>\n",
       "      <td>0.239874</td>\n",
       "      <td>0.410395</td>\n",
       "      <td>0.067645</td>\n",
       "      <td>0.436522</td>\n",
       "      <td>0.121261</td>\n",
       "      <td>0.548237</td>\n",
       "      <td>0.501294</td>\n",
       "      <td>0.283304</td>\n",
       "      <td>0.416698</td>\n",
       "      <td>0.372548</td>\n",
       "      <td>0.353712</td>\n",
       "      <td>0.374357</td>\n",
       "      <td>0.262839</td>\n",
       "      <td>0.212521</td>\n",
       "      <td>0.317400</td>\n",
       "      <td>0.559551</td>\n",
       "      <td>0.376199</td>\n",
       "      <td>0.181805</td>\n",
       "      <td>0.258720</td>\n",
       "      <td>0.337126</td>\n",
       "      <td>0.079697</td>\n",
       "      <td>0.330219</td>\n",
       "      <td>0.686524</td>\n",
       "      <td>0.410187</td>\n",
       "      <td>0.385008</td>\n",
       "      <td>0.422195</td>\n",
       "      <td>0.080162</td>\n",
       "      <td>0.234762</td>\n",
       "      <td>0.656300</td>\n",
       "      <td>0.147044</td>\n",
       "      <td>0.325597</td>\n",
       "      <td>0.187625</td>\n",
       "      <td>0.393332</td>\n",
       "      <td>0.480334</td>\n",
       "      <td>0.116965</td>\n",
       "      <td>0.215528</td>\n",
       "      <td>0.480342</td>\n",
       "      <td>0.245702</td>\n",
       "      <td>0.215008</td>\n",
       "      <td>0.269902</td>\n",
       "      <td>0.147711</td>\n",
       "      <td>0.350381</td>\n",
       "      <td>0.096959</td>\n",
       "      <td>0.136298</td>\n",
       "      <td>0.149281</td>\n",
       "      <td>0.812053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  MouseID  cls  DYRK1A_N    BDNF_N     NR1_N    NR2A_N    pAKT_N   pBRAF_N  \\\n",
       "0   309_1    0  0.151122  0.824638  0.612119  0.630482  0.327006  0.448666   \n",
       "1   309_2    0  0.155750  0.776455  0.601070  0.585247  0.311887  0.429899   \n",
       "2   309_3    0  0.153459  0.793572  0.558911  0.575910  0.306369  0.441381   \n",
       "3   309_4    0  0.125169  0.637326  0.468152  0.480646  0.335530  0.444307   \n",
       "4   309_5    0  0.122146  0.637787  0.426467  0.441977  0.314976  0.433100   \n",
       "\n",
       "   pCAMKII_N   pCREB_N    pELK_N    pJNK_N    PKCA_N    pMEK_N   pNR2A_N  \\\n",
       "0   0.168257  0.617322  0.232553  0.576167  0.747688  0.598504  0.286697   \n",
       "1   0.154925  0.590173  0.205362  0.559556  0.688545  0.559598  0.270688   \n",
       "2   0.153485  0.607102  0.199194  0.541938  0.670905  0.560573  0.283848   \n",
       "3   0.132074  0.486945  0.205135  0.554293  0.657048  0.640641  0.211238   \n",
       "4   0.129086  0.410194  0.189152  0.532161  0.608970  0.551236  0.200364   \n",
       "\n",
       "   pPKCAB_N    pRSK_N     AKT_N  CAMKII_N    CREB_N     ELK_N     ERK_N  \\\n",
       "0  0.698164  0.622784  0.711198  0.419649  0.317149  0.593715  0.627907   \n",
       "1  0.591606  0.629311  0.689095  0.383669  0.291583  0.548032  0.578736   \n",
       "2  0.581558  0.669753  0.670907  0.415579  0.292676  0.549983  0.599925   \n",
       "3  0.627585  0.687775  0.593408  0.401866  0.319599  0.342080  0.452058   \n",
       "4  0.579839  0.698127  0.557701  0.381348  0.297196  0.358746  0.433909   \n",
       "\n",
       "    GSK3B_N     JNK_N     MEK_N    TRKA_N     RSK_N     APP_N    SOD1_N  \\\n",
       "0  0.596269  0.640167  0.643070  0.766146  0.295596  0.549817  0.092035   \n",
       "1  0.584234  0.614355  0.586180  0.724593  0.251840  0.491969  0.075589   \n",
       "2  0.580790  0.625756  0.613502  0.730384  0.270560  0.472444  0.076445   \n",
       "3  0.545716  0.625528  0.492220  0.667283  0.277208  0.440780  0.076936   \n",
       "4  0.519977  0.599623  0.471524  0.627685  0.239874  0.410395  0.067645   \n",
       "\n",
       "     MTOR_N     P38_N   pMTOR_N   DSCR1_N   AMPKA_N    NR2B_N   pNUMB_N  \\\n",
       "0  0.541196  0.152338  0.687269  0.553922  0.467278  0.509996  0.469461   \n",
       "1  0.467650  0.137451  0.621033  0.512117  0.409897  0.457694  0.410046   \n",
       "2  0.470688  0.137475  0.616142  0.510175  0.375653  0.467725  0.400225   \n",
       "3  0.479533  0.144945  0.605654  0.514305  0.338199  0.461189  0.406715   \n",
       "4  0.436522  0.121261  0.548237  0.501294  0.283304  0.416698  0.372548   \n",
       "\n",
       "   RAPTOR_N   TIAM1_N  pP70S6_N    NUMB_N   P70S6_N  pGSK3B_N   pPKCG_N  \\\n",
       "0  0.436170  0.506018  0.163368  0.323059  0.373254  0.604516  0.303363   \n",
       "1  0.383101  0.447496  0.145594  0.322736  0.376914  0.618918  0.302059   \n",
       "2  0.356363  0.432379  0.125773  0.334325  0.383316  0.657565  0.332565   \n",
       "3  0.402786  0.422989  0.267936  0.220414  0.311501  0.549444  0.364189   \n",
       "4  0.353712  0.374357  0.262839  0.212521  0.317400  0.559551  0.376199   \n",
       "\n",
       "     CDK5_N      S6_N  ADARB1_N  AcetylH3K9_N    RRP1_N     BAX_N     ARC_N  \\\n",
       "0  0.178458  0.324085  0.402810      0.083584  0.327870  0.690257  0.426816   \n",
       "1  0.177452  0.324004  0.386524      0.084514  0.326426  0.664894  0.429952   \n",
       "2  0.189744  0.369553  0.373234      0.094486  0.312439  0.700212  0.448652   \n",
       "3  0.181101  0.231762  0.333033      0.076248  0.338264  0.669358  0.392700   \n",
       "4  0.181805  0.258720  0.337126      0.079697  0.330219  0.686524  0.410187   \n",
       "\n",
       "    ERBB4_N    nNOS_N     Tau_N    GFAP_N   GluR3_N   GluR4_N    IL1B_N  \\\n",
       "0  0.412721  0.477834  0.057168  0.228825  0.531163  0.151103  0.242608   \n",
       "1  0.463251  0.488028  0.075103  0.251911  0.576828  0.149555  0.285860   \n",
       "2  0.415863  0.475009  0.071718  0.248192  0.607535  0.150434  0.373879   \n",
       "3  0.372772  0.397939  0.053257  0.245676  0.562594  0.156083  0.242672   \n",
       "4  0.385008  0.422195  0.080162  0.234762  0.656300  0.147044  0.325597   \n",
       "\n",
       "    P3525_N  pCASP9_N   PSD95_N    SNCA_N  Ubiquitin_N  pGSK3B_Tyr216_N  \\\n",
       "0  0.169702  0.432843  0.483783  0.044770     0.256699         0.405228   \n",
       "1  0.212421  0.472327  0.477640  0.054452     0.226088         0.433471   \n",
       "2  0.202733  0.467603  0.484953  0.044526     0.214719         0.429387   \n",
       "3  0.184790  0.364359  0.449304  0.119259     0.208943         0.407971   \n",
       "4  0.187625  0.393332  0.480334  0.116965     0.215528         0.480342   \n",
       "\n",
       "      SHH_N     BAD_N    BCL2_N   pCFOS_N     SYP_N  H3AcK18_N    EGR1_N  \\\n",
       "0  0.162941  0.177312  0.269902  0.133930  0.336299   0.087715  0.102890   \n",
       "1  0.220010  0.146494  0.269902  0.110434  0.365208   0.080692  0.115874   \n",
       "2  0.186816  0.155920  0.269902  0.121560  0.353621   0.080465  0.109050   \n",
       "3  0.179047  0.229602  0.269902  0.151031  0.265619   0.126763  0.164241   \n",
       "4  0.245702  0.215008  0.269902  0.147711  0.350381   0.096959  0.136298   \n",
       "\n",
       "   H3MeK4_N    CaNA_N  \n",
       "0  0.084580  0.705738  \n",
       "1  0.093977  0.749771  \n",
       "2  0.082162  0.868229  \n",
       "3  0.144543  0.721879  \n",
       "4  0.149281  0.812053  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset=pd.read_csv(\"data.csv\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1080 entries, 0 to 1079\n",
      "Data columns (total 72 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   MouseID          1080 non-null   object \n",
      " 1   cls              1080 non-null   int64  \n",
      " 2   DYRK1A_N         1080 non-null   float64\n",
      " 3   BDNF_N           1080 non-null   float64\n",
      " 4   NR1_N            1080 non-null   float64\n",
      " 5   NR2A_N           1080 non-null   float64\n",
      " 6   pAKT_N           1080 non-null   float64\n",
      " 7   pBRAF_N          1080 non-null   float64\n",
      " 8   pCAMKII_N        1080 non-null   float64\n",
      " 9   pCREB_N          1080 non-null   float64\n",
      " 10  pELK_N           1080 non-null   float64\n",
      " 11  pJNK_N           1080 non-null   float64\n",
      " 12  PKCA_N           1080 non-null   float64\n",
      " 13  pMEK_N           1080 non-null   float64\n",
      " 14  pNR2A_N          1080 non-null   float64\n",
      " 15  pPKCAB_N         1080 non-null   float64\n",
      " 16  pRSK_N           1080 non-null   float64\n",
      " 17  AKT_N            1080 non-null   float64\n",
      " 18  CAMKII_N         1080 non-null   float64\n",
      " 19  CREB_N           1080 non-null   float64\n",
      " 20  ELK_N            1080 non-null   float64\n",
      " 21  ERK_N            1080 non-null   float64\n",
      " 22  GSK3B_N          1080 non-null   float64\n",
      " 23  JNK_N            1080 non-null   float64\n",
      " 24  MEK_N            1080 non-null   float64\n",
      " 25  TRKA_N           1080 non-null   float64\n",
      " 26  RSK_N            1080 non-null   float64\n",
      " 27  APP_N            1080 non-null   float64\n",
      " 28  SOD1_N           1080 non-null   float64\n",
      " 29  MTOR_N           1080 non-null   float64\n",
      " 30  P38_N            1080 non-null   float64\n",
      " 31  pMTOR_N          1080 non-null   float64\n",
      " 32  DSCR1_N          1080 non-null   float64\n",
      " 33  AMPKA_N          1080 non-null   float64\n",
      " 34  NR2B_N           1080 non-null   float64\n",
      " 35  pNUMB_N          1080 non-null   float64\n",
      " 36  RAPTOR_N         1080 non-null   float64\n",
      " 37  TIAM1_N          1080 non-null   float64\n",
      " 38  pP70S6_N         1080 non-null   float64\n",
      " 39  NUMB_N           1080 non-null   float64\n",
      " 40  P70S6_N          1080 non-null   float64\n",
      " 41  pGSK3B_N         1080 non-null   float64\n",
      " 42  pPKCG_N          1080 non-null   float64\n",
      " 43  CDK5_N           1080 non-null   float64\n",
      " 44  S6_N             1080 non-null   float64\n",
      " 45  ADARB1_N         1080 non-null   float64\n",
      " 46  AcetylH3K9_N     1080 non-null   float64\n",
      " 47  RRP1_N           1080 non-null   float64\n",
      " 48  BAX_N            1080 non-null   float64\n",
      " 49  ARC_N            1080 non-null   float64\n",
      " 50  ERBB4_N          1080 non-null   float64\n",
      " 51  nNOS_N           1080 non-null   float64\n",
      " 52  Tau_N            1080 non-null   float64\n",
      " 53  GFAP_N           1080 non-null   float64\n",
      " 54  GluR3_N          1080 non-null   float64\n",
      " 55  GluR4_N          1080 non-null   float64\n",
      " 56  IL1B_N           1080 non-null   float64\n",
      " 57  P3525_N          1080 non-null   float64\n",
      " 58  pCASP9_N         1080 non-null   float64\n",
      " 59  PSD95_N          1080 non-null   float64\n",
      " 60  SNCA_N           1080 non-null   float64\n",
      " 61  Ubiquitin_N      1080 non-null   float64\n",
      " 62  pGSK3B_Tyr216_N  1080 non-null   float64\n",
      " 63  SHH_N            1080 non-null   float64\n",
      " 64  BAD_N            1080 non-null   float64\n",
      " 65  BCL2_N           1080 non-null   float64\n",
      " 66  pCFOS_N          1080 non-null   float64\n",
      " 67  SYP_N            1080 non-null   float64\n",
      " 68  H3AcK18_N        1080 non-null   float64\n",
      " 69  EGR1_N           1080 non-null   float64\n",
      " 70  H3MeK4_N         1080 non-null   float64\n",
      " 71  CaNA_N           1080 non-null   float64\n",
      "dtypes: float64(70), int64(1), object(1)\n",
      "memory usage: 607.6+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=dataset.drop(labels=['cls','MouseID'], axis=1)\n",
    "y=dataset['cls']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,y,\n",
    "    test_size=0.3,\n",
    "    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from catboost import CatBoostClassifier \n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import (accuracy_score,classification_report,confusion_matrix,\n",
    "                             precision_score,recall_score,f1_score,roc_auc_score,mean_squared_error,mean_absolute_error,r2_score,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "models={\n",
    "    \"Decison Tree\":DecisionTreeClassifier(),\n",
    "    \"Random Forest\":RandomForestClassifier(),\n",
    "    \"XGBoost\":xgb.XGBClassifier(objective=\"multi:softprob\", random_state=42),\n",
    "    \"GradientBoost\":GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=1, random_state=0),\n",
    "    \"AdaBoost\":AdaBoostClassifier(n_estimators=100, random_state=0),\n",
    "    \"CatBoost\":CatBoostClassifier(learning_rate= 0.1, depth=6, l2_leaf_reg= 3, iterations= 100)\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "============================================================\n",
      "Decison Tree\n",
      "Model Performance for Training set\n",
      "- Accuracy:1.0000\n",
      "- F1 Score:1.0000\n",
      "- Precision:1.0000\n",
      "- Recall:1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       113\n",
      "           1       1.00      1.00      1.00        96\n",
      "           2       1.00      1.00      1.00       107\n",
      "           3       1.00      1.00      1.00        91\n",
      "           4       1.00      1.00      1.00        85\n",
      "           5       1.00      1.00      1.00        71\n",
      "           6       1.00      1.00      1.00       100\n",
      "           7       1.00      1.00      1.00        93\n",
      "\n",
      "    accuracy                           1.00       756\n",
      "   macro avg       1.00      1.00      1.00       756\n",
      "weighted avg       1.00      1.00      1.00       756\n",
      "\n",
      "------------------------------------------------------------\n",
      "Model Performance for Test set\n",
      "- Accuracy:0.8333\n",
      "- F1 Score:0.8349\n",
      "- Precision:0.8440\n",
      "- Recall:0.8333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.78      0.73        37\n",
      "           1       0.67      0.79      0.73        39\n",
      "           2       0.97      0.77      0.86        43\n",
      "           3       0.82      0.91      0.86        44\n",
      "           4       0.86      0.72      0.78        50\n",
      "           5       0.86      0.88      0.87        34\n",
      "           6       0.89      0.89      0.89        35\n",
      "           7       0.98      0.95      0.96        42\n",
      "\n",
      "    accuracy                           0.83       324\n",
      "   macro avg       0.84      0.84      0.84       324\n",
      "weighted avg       0.84      0.83      0.83       324\n",
      "\n",
      "============================================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "============================================================\n",
      "Random Forest\n",
      "Model Performance for Training set\n",
      "- Accuracy:1.0000\n",
      "- F1 Score:1.0000\n",
      "- Precision:1.0000\n",
      "- Recall:1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       113\n",
      "           1       1.00      1.00      1.00        96\n",
      "           2       1.00      1.00      1.00       107\n",
      "           3       1.00      1.00      1.00        91\n",
      "           4       1.00      1.00      1.00        85\n",
      "           5       1.00      1.00      1.00        71\n",
      "           6       1.00      1.00      1.00       100\n",
      "           7       1.00      1.00      1.00        93\n",
      "\n",
      "    accuracy                           1.00       756\n",
      "   macro avg       1.00      1.00      1.00       756\n",
      "weighted avg       1.00      1.00      1.00       756\n",
      "\n",
      "------------------------------------------------------------\n",
      "Model Performance for Test set\n",
      "- Accuracy:0.9599\n",
      "- F1 Score:0.9599\n",
      "- Precision:0.9631\n",
      "- Recall:0.9599\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97        37\n",
      "           1       0.84      0.95      0.89        39\n",
      "           2       0.93      0.98      0.95        43\n",
      "           3       0.98      1.00      0.99        44\n",
      "           4       1.00      0.86      0.92        50\n",
      "           5       1.00      1.00      1.00        34\n",
      "           6       1.00      0.91      0.96        35\n",
      "           7       1.00      1.00      1.00        42\n",
      "\n",
      "    accuracy                           0.96       324\n",
      "   macro avg       0.96      0.96      0.96       324\n",
      "weighted avg       0.96      0.96      0.96       324\n",
      "\n",
      "============================================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "============================================================\n",
      "XGBoost\n",
      "Model Performance for Training set\n",
      "- Accuracy:1.0000\n",
      "- F1 Score:1.0000\n",
      "- Precision:1.0000\n",
      "- Recall:1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       113\n",
      "           1       1.00      1.00      1.00        96\n",
      "           2       1.00      1.00      1.00       107\n",
      "           3       1.00      1.00      1.00        91\n",
      "           4       1.00      1.00      1.00        85\n",
      "           5       1.00      1.00      1.00        71\n",
      "           6       1.00      1.00      1.00       100\n",
      "           7       1.00      1.00      1.00        93\n",
      "\n",
      "    accuracy                           1.00       756\n",
      "   macro avg       1.00      1.00      1.00       756\n",
      "weighted avg       1.00      1.00      1.00       756\n",
      "\n",
      "------------------------------------------------------------\n",
      "Model Performance for Test set\n",
      "- Accuracy:0.9352\n",
      "- F1 Score:0.9355\n",
      "- Precision:0.9391\n",
      "- Recall:0.9352\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.95      0.96        37\n",
      "           1       0.83      1.00      0.91        39\n",
      "           2       0.87      0.93      0.90        43\n",
      "           3       1.00      1.00      1.00        44\n",
      "           4       0.91      0.86      0.89        50\n",
      "           5       1.00      0.91      0.95        34\n",
      "           6       0.94      0.86      0.90        35\n",
      "           7       1.00      0.98      0.99        42\n",
      "\n",
      "    accuracy                           0.94       324\n",
      "   macro avg       0.94      0.94      0.94       324\n",
      "weighted avg       0.94      0.94      0.94       324\n",
      "\n",
      "============================================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "============================================================\n",
      "GradientBoost\n",
      "Model Performance for Training set\n",
      "- Accuracy:1.0000\n",
      "- F1 Score:1.0000\n",
      "- Precision:1.0000\n",
      "- Recall:1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       113\n",
      "           1       1.00      1.00      1.00        96\n",
      "           2       1.00      1.00      1.00       107\n",
      "           3       1.00      1.00      1.00        91\n",
      "           4       1.00      1.00      1.00        85\n",
      "           5       1.00      1.00      1.00        71\n",
      "           6       1.00      1.00      1.00       100\n",
      "           7       1.00      1.00      1.00        93\n",
      "\n",
      "    accuracy                           1.00       756\n",
      "   macro avg       1.00      1.00      1.00       756\n",
      "weighted avg       1.00      1.00      1.00       756\n",
      "\n",
      "------------------------------------------------------------\n",
      "Model Performance for Test set\n",
      "- Accuracy:0.9383\n",
      "- F1 Score:0.9381\n",
      "- Precision:0.9405\n",
      "- Recall:0.9383\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.97      0.94        37\n",
      "           1       0.86      0.95      0.90        39\n",
      "           2       0.93      0.95      0.94        43\n",
      "           3       0.94      1.00      0.97        44\n",
      "           4       0.93      0.86      0.90        50\n",
      "           5       1.00      0.94      0.97        34\n",
      "           6       0.97      0.86      0.91        35\n",
      "           7       1.00      0.98      0.99        42\n",
      "\n",
      "    accuracy                           0.94       324\n",
      "   macro avg       0.94      0.94      0.94       324\n",
      "weighted avg       0.94      0.94      0.94       324\n",
      "\n",
      "============================================================\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\win10\\.conda\\envs\\mice\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\win10\\.conda\\envs\\mice\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\win10\\.conda\\envs\\mice\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\win10\\.conda\\envs\\mice\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\win10\\.conda\\envs\\mice\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\win10\\.conda\\envs\\mice\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\win10\\.conda\\envs\\mice\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\win10\\.conda\\envs\\mice\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "============================================================\n",
      "AdaBoost\n",
      "Model Performance for Training set\n",
      "- Accuracy:0.3704\n",
      "- F1 Score:0.2913\n",
      "- Precision:0.4131\n",
      "- Recall:0.3704\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.56      0.52       113\n",
      "           1       0.66      0.20      0.30        96\n",
      "           2       0.28      1.00      0.44       107\n",
      "           3       0.00      0.00      0.00        91\n",
      "           4       0.56      0.41      0.47        85\n",
      "           5       0.34      0.69      0.46        71\n",
      "           6       0.00      0.00      0.00       100\n",
      "           7       1.00      0.08      0.14        93\n",
      "\n",
      "    accuracy                           0.37       756\n",
      "   macro avg       0.41      0.37      0.29       756\n",
      "weighted avg       0.41      0.37      0.29       756\n",
      "\n",
      "------------------------------------------------------------\n",
      "Model Performance for Test set\n",
      "- Accuracy:0.3086\n",
      "- F1 Score:0.2475\n",
      "- Precision:0.2949\n",
      "- Recall:0.3086\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.38      0.34        37\n",
      "           1       0.48      0.26      0.33        39\n",
      "           2       0.24      0.86      0.38        43\n",
      "           3       0.00      0.00      0.00        44\n",
      "           4       0.58      0.36      0.44        50\n",
      "           5       0.28      0.56      0.37        34\n",
      "           6       0.00      0.00      0.00        35\n",
      "           7       0.40      0.05      0.09        42\n",
      "\n",
      "    accuracy                           0.31       324\n",
      "   macro avg       0.29      0.31      0.24       324\n",
      "weighted avg       0.29      0.31      0.25       324\n",
      "\n",
      "============================================================\n",
      "\n",
      "\n",
      "0:\tlearn: 1.9241547\ttotal: 504ms\tremaining: 49.9s\n",
      "1:\tlearn: 1.7965522\ttotal: 842ms\tremaining: 41.3s\n",
      "2:\tlearn: 1.6961722\ttotal: 1.35s\tremaining: 43.5s\n",
      "3:\tlearn: 1.5919818\ttotal: 1.78s\tremaining: 42.8s\n",
      "4:\tlearn: 1.5012041\ttotal: 1.96s\tremaining: 37.3s\n",
      "5:\tlearn: 1.4262966\ttotal: 2.3s\tremaining: 36.1s\n",
      "6:\tlearn: 1.3495608\ttotal: 2.67s\tremaining: 35.5s\n",
      "7:\tlearn: 1.2755493\ttotal: 3.06s\tremaining: 35.2s\n",
      "8:\tlearn: 1.2114026\ttotal: 3.2s\tremaining: 32.4s\n",
      "9:\tlearn: 1.1524381\ttotal: 3.35s\tremaining: 30.1s\n",
      "10:\tlearn: 1.1163838\ttotal: 3.51s\tremaining: 28.4s\n",
      "11:\tlearn: 1.0735897\ttotal: 3.69s\tremaining: 27s\n",
      "12:\tlearn: 1.0306429\ttotal: 3.82s\tremaining: 25.6s\n",
      "13:\tlearn: 0.9821674\ttotal: 3.97s\tremaining: 24.4s\n",
      "14:\tlearn: 0.9409387\ttotal: 4.13s\tremaining: 23.4s\n",
      "15:\tlearn: 0.9074920\ttotal: 4.34s\tremaining: 22.8s\n",
      "16:\tlearn: 0.8733000\ttotal: 4.51s\tremaining: 22s\n",
      "17:\tlearn: 0.8414105\ttotal: 4.68s\tremaining: 21.3s\n",
      "18:\tlearn: 0.8164075\ttotal: 4.83s\tremaining: 20.6s\n",
      "19:\tlearn: 0.7919728\ttotal: 4.98s\tremaining: 19.9s\n",
      "20:\tlearn: 0.7650286\ttotal: 5.11s\tremaining: 19.2s\n",
      "21:\tlearn: 0.7406886\ttotal: 5.27s\tremaining: 18.7s\n",
      "22:\tlearn: 0.7162707\ttotal: 5.42s\tremaining: 18.1s\n",
      "23:\tlearn: 0.6879143\ttotal: 5.57s\tremaining: 17.6s\n",
      "24:\tlearn: 0.6664654\ttotal: 5.7s\tremaining: 17.1s\n",
      "25:\tlearn: 0.6465066\ttotal: 5.84s\tremaining: 16.6s\n",
      "26:\tlearn: 0.6291539\ttotal: 5.97s\tremaining: 16.2s\n",
      "27:\tlearn: 0.6130561\ttotal: 6.12s\tremaining: 15.7s\n",
      "28:\tlearn: 0.5983394\ttotal: 6.27s\tremaining: 15.4s\n",
      "29:\tlearn: 0.5821804\ttotal: 6.41s\tremaining: 15s\n",
      "30:\tlearn: 0.5663642\ttotal: 6.54s\tremaining: 14.6s\n",
      "31:\tlearn: 0.5524134\ttotal: 6.68s\tremaining: 14.2s\n",
      "32:\tlearn: 0.5388858\ttotal: 6.82s\tremaining: 13.8s\n",
      "33:\tlearn: 0.5279663\ttotal: 6.96s\tremaining: 13.5s\n",
      "34:\tlearn: 0.5157679\ttotal: 7.1s\tremaining: 13.2s\n",
      "35:\tlearn: 0.5047147\ttotal: 7.25s\tremaining: 12.9s\n",
      "36:\tlearn: 0.4907124\ttotal: 7.39s\tremaining: 12.6s\n",
      "37:\tlearn: 0.4799715\ttotal: 7.54s\tremaining: 12.3s\n",
      "38:\tlearn: 0.4688768\ttotal: 7.67s\tremaining: 12s\n",
      "39:\tlearn: 0.4540707\ttotal: 7.83s\tremaining: 11.8s\n",
      "40:\tlearn: 0.4441969\ttotal: 7.99s\tremaining: 11.5s\n",
      "41:\tlearn: 0.4337281\ttotal: 8.13s\tremaining: 11.2s\n",
      "42:\tlearn: 0.4259284\ttotal: 8.26s\tremaining: 11s\n",
      "43:\tlearn: 0.4157435\ttotal: 8.41s\tremaining: 10.7s\n",
      "44:\tlearn: 0.4066182\ttotal: 8.56s\tremaining: 10.5s\n",
      "45:\tlearn: 0.3986860\ttotal: 8.72s\tremaining: 10.2s\n",
      "46:\tlearn: 0.3898411\ttotal: 8.85s\tremaining: 9.98s\n",
      "47:\tlearn: 0.3808217\ttotal: 9.03s\tremaining: 9.78s\n",
      "48:\tlearn: 0.3720632\ttotal: 9.2s\tremaining: 9.57s\n",
      "49:\tlearn: 0.3630663\ttotal: 9.36s\tremaining: 9.36s\n",
      "50:\tlearn: 0.3552963\ttotal: 9.49s\tremaining: 9.12s\n",
      "51:\tlearn: 0.3475108\ttotal: 9.64s\tremaining: 8.9s\n",
      "52:\tlearn: 0.3413283\ttotal: 9.85s\tremaining: 8.73s\n",
      "53:\tlearn: 0.3346743\ttotal: 10s\tremaining: 8.53s\n",
      "54:\tlearn: 0.3289861\ttotal: 10.2s\tremaining: 8.32s\n",
      "55:\tlearn: 0.3222492\ttotal: 10.4s\tremaining: 8.14s\n",
      "56:\tlearn: 0.3151172\ttotal: 10.5s\tremaining: 7.91s\n",
      "57:\tlearn: 0.3103378\ttotal: 10.6s\tremaining: 7.7s\n",
      "58:\tlearn: 0.3058777\ttotal: 10.8s\tremaining: 7.49s\n",
      "59:\tlearn: 0.3000230\ttotal: 10.9s\tremaining: 7.27s\n",
      "60:\tlearn: 0.2938763\ttotal: 11s\tremaining: 7.06s\n",
      "61:\tlearn: 0.2894171\ttotal: 11.2s\tremaining: 6.86s\n",
      "62:\tlearn: 0.2853797\ttotal: 11.3s\tremaining: 6.66s\n",
      "63:\tlearn: 0.2803233\ttotal: 11.5s\tremaining: 6.45s\n",
      "64:\tlearn: 0.2765780\ttotal: 11.6s\tremaining: 6.26s\n",
      "65:\tlearn: 0.2709348\ttotal: 11.8s\tremaining: 6.06s\n",
      "66:\tlearn: 0.2664320\ttotal: 11.9s\tremaining: 5.86s\n",
      "67:\tlearn: 0.2605290\ttotal: 12s\tremaining: 5.66s\n",
      "68:\tlearn: 0.2560238\ttotal: 12.2s\tremaining: 5.46s\n",
      "69:\tlearn: 0.2527430\ttotal: 12.3s\tremaining: 5.28s\n",
      "70:\tlearn: 0.2478963\ttotal: 12.5s\tremaining: 5.09s\n",
      "71:\tlearn: 0.2451647\ttotal: 12.6s\tremaining: 4.9s\n",
      "72:\tlearn: 0.2419629\ttotal: 12.7s\tremaining: 4.71s\n",
      "73:\tlearn: 0.2366034\ttotal: 12.9s\tremaining: 4.53s\n",
      "74:\tlearn: 0.2325152\ttotal: 13s\tremaining: 4.34s\n",
      "75:\tlearn: 0.2295946\ttotal: 13.2s\tremaining: 4.15s\n",
      "76:\tlearn: 0.2253821\ttotal: 13.3s\tremaining: 3.97s\n",
      "77:\tlearn: 0.2221415\ttotal: 13.4s\tremaining: 3.79s\n",
      "78:\tlearn: 0.2190924\ttotal: 13.6s\tremaining: 3.61s\n",
      "79:\tlearn: 0.2157555\ttotal: 13.7s\tremaining: 3.43s\n",
      "80:\tlearn: 0.2138358\ttotal: 13.9s\tremaining: 3.25s\n",
      "81:\tlearn: 0.2101354\ttotal: 14s\tremaining: 3.07s\n",
      "82:\tlearn: 0.2068933\ttotal: 14.1s\tremaining: 2.9s\n",
      "83:\tlearn: 0.2043093\ttotal: 14.3s\tremaining: 2.72s\n",
      "84:\tlearn: 0.2014223\ttotal: 14.4s\tremaining: 2.54s\n",
      "85:\tlearn: 0.1985495\ttotal: 14.5s\tremaining: 2.37s\n",
      "86:\tlearn: 0.1942142\ttotal: 14.7s\tremaining: 2.19s\n",
      "87:\tlearn: 0.1925208\ttotal: 14.8s\tremaining: 2.02s\n",
      "88:\tlearn: 0.1895966\ttotal: 15s\tremaining: 1.85s\n",
      "89:\tlearn: 0.1871887\ttotal: 15.1s\tremaining: 1.68s\n",
      "90:\tlearn: 0.1841451\ttotal: 15.2s\tremaining: 1.51s\n",
      "91:\tlearn: 0.1810350\ttotal: 15.4s\tremaining: 1.34s\n",
      "92:\tlearn: 0.1785177\ttotal: 15.5s\tremaining: 1.17s\n",
      "93:\tlearn: 0.1756710\ttotal: 15.7s\tremaining: 1s\n",
      "94:\tlearn: 0.1733958\ttotal: 15.8s\tremaining: 833ms\n",
      "95:\tlearn: 0.1707287\ttotal: 16s\tremaining: 667ms\n",
      "96:\tlearn: 0.1680447\ttotal: 16.2s\tremaining: 502ms\n",
      "97:\tlearn: 0.1661362\ttotal: 16.4s\tremaining: 334ms\n",
      "98:\tlearn: 0.1634133\ttotal: 16.5s\tremaining: 167ms\n",
      "99:\tlearn: 0.1614574\ttotal: 16.6s\tremaining: 0us\n",
      "\n",
      "\n",
      "============================================================\n",
      "CatBoost\n",
      "Model Performance for Training set\n",
      "- Accuracy:1.0000\n",
      "- F1 Score:1.0000\n",
      "- Precision:1.0000\n",
      "- Recall:1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       113\n",
      "           1       1.00      1.00      1.00        96\n",
      "           2       1.00      1.00      1.00       107\n",
      "           3       1.00      1.00      1.00        91\n",
      "           4       1.00      1.00      1.00        85\n",
      "           5       1.00      1.00      1.00        71\n",
      "           6       1.00      1.00      1.00       100\n",
      "           7       1.00      1.00      1.00        93\n",
      "\n",
      "    accuracy                           1.00       756\n",
      "   macro avg       1.00      1.00      1.00       756\n",
      "weighted avg       1.00      1.00      1.00       756\n",
      "\n",
      "------------------------------------------------------------\n",
      "Model Performance for Test set\n",
      "- Accuracy:0.9383\n",
      "- F1 Score:0.9384\n",
      "- Precision:0.9428\n",
      "- Recall:0.9383\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.97      0.91        37\n",
      "           1       0.82      0.92      0.87        39\n",
      "           2       0.93      0.98      0.95        43\n",
      "           3       0.98      1.00      0.99        44\n",
      "           4       0.95      0.82      0.88        50\n",
      "           5       1.00      0.91      0.95        34\n",
      "           6       1.00      0.91      0.96        35\n",
      "           7       1.00      1.00      1.00        42\n",
      "\n",
      "    accuracy                           0.94       324\n",
      "   macro avg       0.94      0.94      0.94       324\n",
      "weighted avg       0.94      0.94      0.94       324\n",
      "\n",
      "============================================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(list(models))):\n",
    "    model=list(models.values())[i]\n",
    "    model.fit(X_train,y_train)\n",
    "    \n",
    "    y_train_pred=model.predict(X_train)\n",
    "    y_test_pred=model.predict(X_test)\n",
    "    \n",
    "    #training   performance\n",
    "    model_train_accuracy=accuracy_score(y_train,y_train_pred)\n",
    "    model_train_f1=f1_score(y_train,y_train_pred,average='weighted')\n",
    "    model_train_precission=precision_score(y_train,y_train_pred,average='weighted')\n",
    "    model_trian_recall=recall_score(y_train,y_train_pred,average='weighted')\n",
    "    #model_train_roc_auc_score=roc_auc_score(y_train,y_train_pred,multi_class='ovr')\n",
    "    \n",
    "    #training   performance\n",
    "    model_test_accuracy=accuracy_score(y_test,y_test_pred)\n",
    "    model_test_f1=f1_score(y_test,y_test_pred,average='weighted')\n",
    "    model_test_precission=precision_score(y_test,y_test_pred,average='weighted')\n",
    "    model_test_recall=recall_score(y_test,y_test_pred,average='weighted')\n",
    "    #model_test_roc_auc_score=roc_auc_score(y_test,y_test_pred,multi_class='ovo')\n",
    "    \n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"=\"*60)\n",
    "    print(list(models.keys())[i])\n",
    "    print('Model Performance for Training set')\n",
    "    print(\"- Accuracy:{:.4f}\".format(model_train_accuracy))\n",
    "    print(\"- F1 Score:{:.4f}\".format(model_train_f1))\n",
    "    print(\"- Precision:{:.4f}\".format(model_train_precission))\n",
    "    print(\"- Recall:{:.4f}\".format(model_trian_recall))\n",
    "    #print(\"- Roc Auc Score:{:.4f}\".format(model_train_roc_auc_score))\n",
    "    print(classification_report(y_train,y_train_pred))\n",
    "    print(\"-\"*60) \n",
    "    print('Model Performance for Test set')\n",
    "    print(\"- Accuracy:{:.4f}\".format(model_test_accuracy))\n",
    "    print(\"- F1 Score:{:.4f}\".format(model_test_f1))\n",
    "    print(\"- Precision:{:.4f}\".format(model_test_precission))\n",
    "    print(\"- Recall:{:.4f}\".format(model_test_recall))\n",
    "   # print(\"- Roc Auc Score:{:.4f}\".format(model_test_roc_auc_score))\n",
    "    print(classification_report(y_test,y_test_pred))\n",
    "    print(\"=\"*60) \n",
    "    print(\"\\n\")\n",
    "              \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\win10\\.conda\\envs\\mice\\lib\\site-packages\\pydantic\\_internal\\_fields.py:149: UserWarning: Field \"model_server_url\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\win10\\.conda\\envs\\mice\\lib\\site-packages\\pydantic\\_internal\\_config.py:321: UserWarning: Valid config keys have changed in V2:\n",
      "* 'schema_extra' has been renamed to 'json_schema_extra'\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/8d141a8e94454bd28d86c3dd0bf5e3a6', creation_time=1701622844322, experiment_id='1', last_update_time=1701622844322, lifecycle_stage='active', name='Mice Protien Expression', tags={'mlflow.note.content': '\\n'\n",
       "                        'Expression levels of 77 proteins measured in the '\n",
       "                        'cerebral cortex of 8 classes of control and Down '\n",
       "                        'syndrome mice exposed to context fear conditioning, a '\n",
       "                        'task used to assess associative learning.\\n'\n",
       "                        'Dataset Characteristics  :  Multivariate        \\n'\n",
       "                        'Subject Area      :             Classification, '\n",
       "                        'Clustering              \\n'\n",
       "                        'Feature Type    :               Real  \\n'\n",
       "                        'Instances       :                      1080\\n'\n",
       "                        '  \\n'\n",
       "                        '\\n'\n",
       "                        '### [Click here to see Dataset]( '\n",
       "                        'https://archive.ics.uci.edu/dataset/342/mice+protein+expression)                                                                             \\n'\n",
       "                        '\\n'\n",
       "                        '\\n'\n",
       "                        '\\n'\n",
       "                        '\\n'\n",
       "                        '\\n'\n",
       "                        '\\n'\n",
       "                        '\\n'\n",
       "                        '\\n'\n",
       "                        '\\n'\n",
       "                        '\\n'\n",
       "                        '\\n'}>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "mlflow.set_tracking_uri(\"https://dagshub.com/SunilKumar-ugra/Mice-Protein-Expression.mlflow\")\n",
    "mlflow.set_experiment(\"Mice Protien Expression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['Decison Tree', 'Random Forest', 'XGBoost', 'GradientBoost', 'AdaBoost', 'CatBoost']\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=list(models.keys())\n",
    "str(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_metrics(actual, pred):\n",
    "        mse=mean_squared_error(actual, pred)\n",
    "        rmse = np.sqrt(mean_squared_error(actual, pred))\n",
    "        mae = mean_absolute_error(actual, pred)\n",
    "        r2 = r2_score(actual, pred)\n",
    "        return rmse, mae, r2, mse\n",
    "\n",
    "def performace_matric(actual, pred,avg):\n",
    "        acc=accuracy_score(actual,pred)\n",
    "        f1=f1_score(actual,pred,average=avg)\n",
    "        precission=precision_score(actual,pred,average=avg)\n",
    "        recall=recall_score(actual,pred,average=avg)\n",
    "        return acc,f1,precission,recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\win10\\.conda\\envs\\mice\\lib\\site-packages\\_distutils_hack\\__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "c:\\Users\\win10\\.conda\\envs\\mice\\lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "c:\\Users\\win10\\.conda\\envs\\mice\\lib\\site-packages\\_distutils_hack\\__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "c:\\Users\\win10\\.conda\\envs\\mice\\lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "c:\\Users\\win10\\.conda\\envs\\mice\\lib\\site-packages\\_distutils_hack\\__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "c:\\Users\\win10\\.conda\\envs\\mice\\lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "c:\\Users\\win10\\.conda\\envs\\mice\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\win10\\.conda\\envs\\mice\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\win10\\.conda\\envs\\mice\\lib\\site-packages\\_distutils_hack\\__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "c:\\Users\\win10\\.conda\\envs\\mice\\lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 1.9241547\ttotal: 165ms\tremaining: 16.3s\n",
      "1:\tlearn: 1.7965522\ttotal: 296ms\tremaining: 14.5s\n",
      "2:\tlearn: 1.6961722\ttotal: 477ms\tremaining: 15.4s\n",
      "3:\tlearn: 1.5919818\ttotal: 610ms\tremaining: 14.6s\n",
      "4:\tlearn: 1.5012041\ttotal: 751ms\tremaining: 14.3s\n",
      "5:\tlearn: 1.4262966\ttotal: 891ms\tremaining: 14s\n",
      "6:\tlearn: 1.3495608\ttotal: 1.03s\tremaining: 13.6s\n",
      "7:\tlearn: 1.2755493\ttotal: 1.17s\tremaining: 13.5s\n",
      "8:\tlearn: 1.2114026\ttotal: 1.33s\tremaining: 13.4s\n",
      "9:\tlearn: 1.1524381\ttotal: 1.47s\tremaining: 13.2s\n",
      "10:\tlearn: 1.1163838\ttotal: 1.62s\tremaining: 13.1s\n",
      "11:\tlearn: 1.0735897\ttotal: 1.78s\tremaining: 13s\n",
      "12:\tlearn: 1.0306429\ttotal: 1.93s\tremaining: 12.9s\n",
      "13:\tlearn: 0.9821674\ttotal: 2.06s\tremaining: 12.7s\n",
      "14:\tlearn: 0.9409387\ttotal: 2.21s\tremaining: 12.5s\n",
      "15:\tlearn: 0.9074920\ttotal: 2.34s\tremaining: 12.3s\n",
      "16:\tlearn: 0.8733000\ttotal: 2.49s\tremaining: 12.1s\n",
      "17:\tlearn: 0.8414105\ttotal: 2.63s\tremaining: 12s\n",
      "18:\tlearn: 0.8164075\ttotal: 2.78s\tremaining: 11.8s\n",
      "19:\tlearn: 0.7919728\ttotal: 2.92s\tremaining: 11.7s\n",
      "20:\tlearn: 0.7650286\ttotal: 3.08s\tremaining: 11.6s\n",
      "21:\tlearn: 0.7406886\ttotal: 3.25s\tremaining: 11.5s\n",
      "22:\tlearn: 0.7162707\ttotal: 3.41s\tremaining: 11.4s\n",
      "23:\tlearn: 0.6879143\ttotal: 3.55s\tremaining: 11.3s\n",
      "24:\tlearn: 0.6664654\ttotal: 3.68s\tremaining: 11s\n",
      "25:\tlearn: 0.6465066\ttotal: 3.82s\tremaining: 10.9s\n",
      "26:\tlearn: 0.6291539\ttotal: 4.05s\tremaining: 11s\n",
      "27:\tlearn: 0.6130561\ttotal: 4.35s\tremaining: 11.2s\n",
      "28:\tlearn: 0.5983394\ttotal: 4.52s\tremaining: 11.1s\n",
      "29:\tlearn: 0.5821804\ttotal: 4.69s\tremaining: 10.9s\n",
      "30:\tlearn: 0.5663642\ttotal: 4.83s\tremaining: 10.7s\n",
      "31:\tlearn: 0.5524134\ttotal: 4.98s\tremaining: 10.6s\n",
      "32:\tlearn: 0.5388858\ttotal: 5.12s\tremaining: 10.4s\n",
      "33:\tlearn: 0.5279663\ttotal: 5.26s\tremaining: 10.2s\n",
      "34:\tlearn: 0.5157679\ttotal: 5.4s\tremaining: 10s\n",
      "35:\tlearn: 0.5047147\ttotal: 5.55s\tremaining: 9.87s\n",
      "36:\tlearn: 0.4907124\ttotal: 5.68s\tremaining: 9.67s\n",
      "37:\tlearn: 0.4799715\ttotal: 5.82s\tremaining: 9.49s\n",
      "38:\tlearn: 0.4688768\ttotal: 5.95s\tremaining: 9.31s\n",
      "39:\tlearn: 0.4540707\ttotal: 6.1s\tremaining: 9.14s\n",
      "40:\tlearn: 0.4441969\ttotal: 6.23s\tremaining: 8.97s\n",
      "41:\tlearn: 0.4337281\ttotal: 6.38s\tremaining: 8.81s\n",
      "42:\tlearn: 0.4259284\ttotal: 6.5s\tremaining: 8.62s\n",
      "43:\tlearn: 0.4157435\ttotal: 6.64s\tremaining: 8.45s\n",
      "44:\tlearn: 0.4066182\ttotal: 6.77s\tremaining: 8.28s\n",
      "45:\tlearn: 0.3986860\ttotal: 6.96s\tremaining: 8.18s\n",
      "46:\tlearn: 0.3898411\ttotal: 7.12s\tremaining: 8.03s\n",
      "47:\tlearn: 0.3808217\ttotal: 7.27s\tremaining: 7.88s\n",
      "48:\tlearn: 0.3720632\ttotal: 7.4s\tremaining: 7.7s\n",
      "49:\tlearn: 0.3630663\ttotal: 7.54s\tremaining: 7.54s\n",
      "50:\tlearn: 0.3552963\ttotal: 7.67s\tremaining: 7.37s\n",
      "51:\tlearn: 0.3475108\ttotal: 7.85s\tremaining: 7.25s\n",
      "52:\tlearn: 0.3413283\ttotal: 8.01s\tremaining: 7.1s\n",
      "53:\tlearn: 0.3346743\ttotal: 8.19s\tremaining: 6.97s\n",
      "54:\tlearn: 0.3289861\ttotal: 8.36s\tremaining: 6.84s\n",
      "55:\tlearn: 0.3222492\ttotal: 8.55s\tremaining: 6.72s\n",
      "56:\tlearn: 0.3151172\ttotal: 8.71s\tremaining: 6.57s\n",
      "57:\tlearn: 0.3103378\ttotal: 8.89s\tremaining: 6.44s\n",
      "58:\tlearn: 0.3058777\ttotal: 9.09s\tremaining: 6.32s\n",
      "59:\tlearn: 0.3000230\ttotal: 9.4s\tremaining: 6.26s\n",
      "60:\tlearn: 0.2938763\ttotal: 9.69s\tremaining: 6.2s\n",
      "61:\tlearn: 0.2894171\ttotal: 10s\tremaining: 6.14s\n",
      "62:\tlearn: 0.2853797\ttotal: 10.2s\tremaining: 5.96s\n",
      "63:\tlearn: 0.2803233\ttotal: 10.3s\tremaining: 5.79s\n",
      "64:\tlearn: 0.2765780\ttotal: 10.4s\tremaining: 5.62s\n",
      "65:\tlearn: 0.2709348\ttotal: 10.6s\tremaining: 5.45s\n",
      "66:\tlearn: 0.2664320\ttotal: 10.7s\tremaining: 5.27s\n",
      "67:\tlearn: 0.2605290\ttotal: 10.8s\tremaining: 5.1s\n",
      "68:\tlearn: 0.2560238\ttotal: 11s\tremaining: 4.93s\n",
      "69:\tlearn: 0.2527430\ttotal: 11.1s\tremaining: 4.77s\n",
      "70:\tlearn: 0.2478963\ttotal: 11.3s\tremaining: 4.6s\n",
      "71:\tlearn: 0.2451647\ttotal: 11.4s\tremaining: 4.43s\n",
      "72:\tlearn: 0.2419629\ttotal: 11.5s\tremaining: 4.26s\n",
      "73:\tlearn: 0.2366034\ttotal: 11.7s\tremaining: 4.1s\n",
      "74:\tlearn: 0.2325152\ttotal: 11.8s\tremaining: 3.93s\n",
      "75:\tlearn: 0.2295946\ttotal: 11.9s\tremaining: 3.77s\n",
      "76:\tlearn: 0.2253821\ttotal: 12.1s\tremaining: 3.6s\n",
      "77:\tlearn: 0.2221415\ttotal: 12.2s\tremaining: 3.44s\n",
      "78:\tlearn: 0.2190924\ttotal: 12.3s\tremaining: 3.28s\n",
      "79:\tlearn: 0.2157555\ttotal: 12.5s\tremaining: 3.12s\n",
      "80:\tlearn: 0.2138358\ttotal: 12.6s\tremaining: 2.96s\n",
      "81:\tlearn: 0.2101354\ttotal: 12.8s\tremaining: 2.8s\n",
      "82:\tlearn: 0.2068933\ttotal: 12.9s\tremaining: 2.64s\n",
      "83:\tlearn: 0.2043093\ttotal: 13.1s\tremaining: 2.49s\n",
      "84:\tlearn: 0.2014223\ttotal: 13.2s\tremaining: 2.33s\n",
      "85:\tlearn: 0.1985495\ttotal: 13.3s\tremaining: 2.17s\n",
      "86:\tlearn: 0.1942142\ttotal: 13.5s\tremaining: 2.01s\n",
      "87:\tlearn: 0.1925208\ttotal: 13.6s\tremaining: 1.85s\n",
      "88:\tlearn: 0.1895966\ttotal: 13.7s\tremaining: 1.7s\n",
      "89:\tlearn: 0.1871887\ttotal: 13.9s\tremaining: 1.54s\n",
      "90:\tlearn: 0.1841451\ttotal: 14s\tremaining: 1.39s\n",
      "91:\tlearn: 0.1810350\ttotal: 14.2s\tremaining: 1.23s\n",
      "92:\tlearn: 0.1785177\ttotal: 14.3s\tremaining: 1.07s\n",
      "93:\tlearn: 0.1756710\ttotal: 14.4s\tremaining: 921ms\n",
      "94:\tlearn: 0.1733958\ttotal: 14.6s\tremaining: 766ms\n",
      "95:\tlearn: 0.1707287\ttotal: 14.7s\tremaining: 612ms\n",
      "96:\tlearn: 0.1680447\ttotal: 14.8s\tremaining: 459ms\n",
      "97:\tlearn: 0.1661362\ttotal: 15s\tremaining: 305ms\n",
      "98:\tlearn: 0.1634133\ttotal: 15.1s\tremaining: 152ms\n",
      "99:\tlearn: 0.1614574\ttotal: 15.3s\tremaining: 0us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\win10\\.conda\\envs\\mice\\lib\\site-packages\\_distutils_hack\\__init__.py:18: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "c:\\Users\\win10\\.conda\\envs\\mice\\lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mlflow.sklearn.autolog(disable=True)\n",
    "\n",
    "for i in range(len(list(models))):\n",
    "    model=list(models.values())[i]\n",
    "    model_name=list(models.keys())[i]\n",
    "    \n",
    "    with mlflow.start_run(run_name=str(model_name)):\n",
    "        mlflow.set_tag(\"model_name\", str(model_name))\n",
    "        \n",
    "        model.fit(X_train,y_train)\n",
    "    \n",
    "        y_train_pred=model.predict(X_train)\n",
    "        y_test_pred=model.predict(X_test)\n",
    "        \n",
    "        \n",
    "        #training   performance\n",
    "        (trn_acc,trn_f1,trn_precission,trn_recall)=performace_matric(y_train,y_train_pred,'weighted')\n",
    "        (trn_rmse, trn_mae, trn_r2,trn_mse) = eval_metrics(y_train,y_train_pred)\n",
    "        #testing   performance\n",
    "        (tst_acc,tst_f1,tst_precission,tst_recall)=performace_matric(y_test,y_test_pred,'weighted')\n",
    "        (tst_rmse, tst_mae, tst_r2,tst_mse) = eval_metrics(y_test,y_test_pred)\n",
    "        \n",
    "        \n",
    "        #train performance log\n",
    "        mlflow.log_metric(\"Train Root Mean Square Error\", trn_rmse)\n",
    "        mlflow.log_metric(\"Train Mean Absolute Error\", trn_mae)\n",
    "        mlflow.log_metric(\"Train R-Square\", trn_r2)\n",
    "        mlflow.log_metric(\"Train Mean Square Error\", trn_mse)\n",
    "        mlflow.log_metric(\"Train Accuracy\", trn_acc)\n",
    "        mlflow.log_metric(\"Train F1 Score\",trn_f1)\n",
    "        mlflow.log_metric(\"Train Precision\",trn_precission)\n",
    "        mlflow.log_metric(\"Train Recall\",trn_recall)\n",
    "        \n",
    "        # test performance log\n",
    "        mlflow.log_metric(\"Test Root Mean Square Error\", tst_rmse)\n",
    "        mlflow.log_metric(\"Test Mean Absolute Error\", tst_mae)\n",
    "        mlflow.log_metric(\"Test R-Square\", tst_r2)\n",
    "        mlflow.log_metric(\"Test Mean Square Error\", tst_mse)\n",
    "        mlflow.log_metric(\"Test Accuracy\", tst_acc)\n",
    "        mlflow.log_metric(\"Test F1 Score\",tst_f1)\n",
    "        mlflow.log_metric(\"Test Precision\",tst_precission)\n",
    "        mlflow.log_metric(\"Test Recall\",tst_recall)\n",
    "        \n",
    "        #Model log\n",
    "        mlflow.sklearn.log_model(model, str(model_name)+\"_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Current tracking uri: {mlflow.get_tracking_uri()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
